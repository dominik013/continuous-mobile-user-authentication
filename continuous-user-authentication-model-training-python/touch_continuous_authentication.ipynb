{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "In this step we are going to prepare the data, so we can use it with our HMM to train the parameters  \n",
    "First we are trying to use only the x and y position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from conversion_utils import *\n",
    "\n",
    "def getCombinedStrokeSequencesFromDataFrames(paths, features):\n",
    "    stroke_sequences = []\n",
    "    for path in paths:\n",
    "        strokes = getDataFrameFromPath(path)\n",
    "        strokes = normalizeDataframeColumnsMinMax(strokes, features)\n",
    "        #strokes = standardizeDataframeColumns(strokes, features)\n",
    "        strokes = convertToNumpySequences(strokes, features)\n",
    "\n",
    "        #strokes = getSequencesWhereLengthNot(strokes, 0)\n",
    "        strokes = getSequencesWhereLengthBiggerThan(strokes, 10)\n",
    "        #strokes = getSequencesWhereLengthSmallerThan(strokes, 100)\n",
    "\n",
    "        stroke_sequences = stroke_sequences + strokes\n",
    "\n",
    "    return stroke_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pomegranate import *\n",
    "\n",
    "def trainModel(dataset, features = ['x_position', 'y_position'], max_states = 15):\n",
    "    strokes = getCombinedStrokeSequencesFromDataFrames([dataset], features)\n",
    "    train, test, validate = generateTrainTestValidateSplit(strokes)\n",
    "\n",
    "    print(f'Training best model for dataset {dataset}', end = ' - ')\n",
    "    print(f'Training set size: {len(train)} | Test set size: {len(test)}')\n",
    "\n",
    "    best_model = [0]\n",
    "    max_prob = float('-inf')\n",
    "    for states in range(2, max_states):\n",
    "        improvement, model, success = trainModelWithStates(train, states)\n",
    "\n",
    "        if not success:\n",
    "            continue\n",
    "\n",
    "        log_prob = sum(model.log_probability(sequence) / len(sequence) for sequence in test)\n",
    "        print(f'Trained model with {states} states | Log-probability on test set: {log_prob}')\n",
    "\n",
    "        if log_prob > max_prob:\n",
    "            best_model = [states, model]\n",
    "            max_prob = log_prob\n",
    "\n",
    "    print(f'Finished training model for dataset {dataset} | Best performing: {best_model[0]} states | Log-prob: {max_prob}')\n",
    "\n",
    "    return dataset, model, validate\n",
    "\n",
    "def trainModelWithStates(train, states, number_of_tries = 10):\n",
    "    weights = []\n",
    "    for seq in train:\n",
    "        weights.append(len(seq))\n",
    "\n",
    "    best_model = [float('-inf'), np.nan, False]\n",
    "\n",
    "    for i in range(number_of_tries):\n",
    "        model, history = HiddenMarkovModel.from_samples(MultivariateGaussianDistribution,\n",
    "                n_components = states,\n",
    "                X = train,\n",
    "                # weights = weights,\n",
    "                algorithm = 'baum-welch',\n",
    "                n_init = 10,\n",
    "                return_history = True,\n",
    "                verbose = False)\n",
    "\n",
    "        improvement = history.total_improvement[len(history.total_improvement) - 1]\n",
    "        if not np.isnan(improvement):\n",
    "            if improvement > best_model[0]: ## TODO: Improvement says nothing about quality of the model \n",
    "                best_model = [improvement, model, True]\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "------------------------------------------------\nTraining model with up to 10 states!\n------------------------------------------------\nTraining best model for dataset data/dominik/cua-db.db - Training set size: 823 | Test set size: 235\nTrained model with 2 states | Log-probability on test set: 1078.9897544749379\nTrained model with 3 states | Log-probability on test set: 1182.2445626084066\nTrained model with 4 states | Log-probability on test set: 1293.3782209118413\nTrained model with 5 states | Log-probability on test set: 1316.9269449008189\nTrained model with 6 states | Log-probability on test set: 1351.6059304491635\nTrained model with 7 states | Log-probability on test set: 1352.7796159418083\nTrained model with 8 states | Log-probability on test set: 1368.7977072949038\nTrained model with 9 states | Log-probability on test set: 1419.2854120706927\nFinished training model for dataset data/dominik/cua-db.db | Best performing: 9 states | Log-prob: 1419.2854120706927\nTraining best model for dataset data/sabrina/cua-db.db - Training set size: 134 | Test set size: 38\nTrained model with 2 states | Log-probability on test set: 215.71409173418647\nTrained model with 3 states | Log-probability on test set: 227.2530773028611\nTrained model with 4 states | Log-probability on test set: 236.54594988947406\nTrained model with 5 states | Log-probability on test set: 241.65405929737724\nTrained model with 6 states | Log-probability on test set: 248.33444513852837\nTrained model with 7 states | Log-probability on test set: 251.95240642306166\nTrained model with 8 states | Log-probability on test set: 257.44712855115966\nTrained model with 9 states | Log-probability on test set: 262.2680544416977\nFinished training model for dataset data/sabrina/cua-db.db | Best performing: 9 states | Log-prob: 262.2680544416977\nTraining best model for dataset data/alexander/cua-db.db - Training set size: 335 | Test set size: 95\nTrained model with 2 states | Log-probability on test set: 448.1343272024799\nTrained model with 3 states | Log-probability on test set: 496.09160695943353\nTrained model with 4 states | Log-probability on test set: 525.4808545451206\nTrained model with 5 states | Log-probability on test set: 552.5374893755652\nTrained model with 6 states | Log-probability on test set: 559.7753650593154\nTrained model with 7 states | Log-probability on test set: 574.8209830850234\nTrained model with 8 states | Log-probability on test set: 575.1412696591206\nTrained model with 9 states | Log-probability on test set: 582.9218043694515\nFinished training model for dataset data/alexander/cua-db.db | Best performing: 9 states | Log-prob: 582.9218043694515\nTraining best model for dataset data/manuela/cua-db.db - Training set size: 216 | Test set size: 61\nTrained model with 2 states | Log-probability on test set: 316.8504132700302\nTrained model with 3 states | Log-probability on test set: 340.8750933749636\nTrained model with 4 states | Log-probability on test set: 358.7497805571436\nTrained model with 5 states | Log-probability on test set: 368.3240652026535\nTrained model with 6 states | Log-probability on test set: 382.7091780676038\nTrained model with 7 states | Log-probability on test set: 377.286783017586\nTrained model with 8 states | Log-probability on test set: 405.84294639409893\nTrained model with 9 states | Log-probability on test set: 402.0309571130047\nFinished training model for dataset data/manuela/cua-db.db | Best performing: 8 states | Log-prob: 405.84294639409893\nTraining best model for dataset data/nadine/cua-db.db - Training set size: 316 | Test set size: 90\nTrained model with 2 states | Log-probability on test set: 336.09808690156865\nTrained model with 3 states | Log-probability on test set: 392.3475178446895\nTrained model with 4 states | Log-probability on test set: 428.36684955670376\nTrained model with 5 states | Log-probability on test set: 440.5345015406344\nTrained model with 6 states | Log-probability on test set: 469.6358384699312\nTrained model with 7 states | Log-probability on test set: 496.27468712451576\nTrained model with 8 states | Log-probability on test set: 485.71618478384636\nTrained model with 9 states | Log-probability on test set: 543.9508669213337\nFinished training model for dataset data/nadine/cua-db.db | Best performing: 9 states | Log-prob: 543.9508669213337\nTraining best model for dataset data/oehlboeck/cua-db.db - Training set size: 247 | Test set size: 70\nTrained model with 2 states | Log-probability on test set: 272.10160191111044\nTrained model with 3 states | Log-probability on test set: 327.77571903114233\nTrained model with 4 states | Log-probability on test set: 343.7787746039911\nTrained model with 5 states | Log-probability on test set: 351.53007938673886\nTrained model with 6 states | Log-probability on test set: 364.7977459527857\nTrained model with 7 states | Log-probability on test set: 373.138685867809\nTrained model with 8 states | Log-probability on test set: 383.7817542155028\nTrained model with 9 states | Log-probability on test set: 387.6180264851963\nFinished training model for dataset data/oehlboeck/cua-db.db | Best performing: 9 states | Log-prob: 387.6180264851963\n------------------------------------------------\nNow verifying model with validation set!\n------------------------------------------------\nVerifying data/dominik/cua-db.db\nLog-Probability for data/dominik/cua-db.db\t=\t714.771354797995\nLog-Probability for data/sabrina/cua-db.db\t=\t114.79757358366419\nLog-Probability for data/alexander/cua-db.db\t=\t299.5260773997414\nLog-Probability for data/manuela/cua-db.db\t=\t203.16005949691757\nLog-Probability for data/nadine/cua-db.db\t=\t277.2797526269567\nLog-Probability for data/oehlboeck/cua-db.db\t=\t184.2616909779088\n----------------------------------\nVerifying data/sabrina/cua-db.db\nLog-Probability for data/dominik/cua-db.db\t=\t671.0288130309938\nLog-Probability for data/sabrina/cua-db.db\t=\t101.76042952485743\nLog-Probability for data/alexander/cua-db.db\t=\t282.84897941105\nLog-Probability for data/manuela/cua-db.db\t=\t186.40706453550698\nLog-Probability for data/nadine/cua-db.db\t=\t250.61499539442912\nLog-Probability for data/oehlboeck/cua-db.db\t=\t168.59840512097648\n----------------------------------\nVerifying data/alexander/cua-db.db\nLog-Probability for data/dominik/cua-db.db\t=\t711.6833851130805\nLog-Probability for data/sabrina/cua-db.db\t=\t105.07928384546122\nLog-Probability for data/alexander/cua-db.db\t=\t303.0108019864644\nLog-Probability for data/manuela/cua-db.db\t=\t196.62327397439697\nLog-Probability for data/nadine/cua-db.db\t=\t268.15972179318817\nLog-Probability for data/oehlboeck/cua-db.db\t=\t184.62466460157106\n----------------------------------\nVerifying data/manuela/cua-db.db\nLog-Probability for data/dominik/cua-db.db\t=\t671.1352640444156\nLog-Probability for data/sabrina/cua-db.db\t=\t110.62947377365458\nLog-Probability for data/alexander/cua-db.db\t=\t274.4724299867424\nLog-Probability for data/manuela/cua-db.db\t=\t217.6127695398503\nLog-Probability for data/nadine/cua-db.db\t=\t265.08268982052437\nLog-Probability for data/oehlboeck/cua-db.db\t=\t186.54953304828243\n----------------------------------\nVerifying data/nadine/cua-db.db\nLog-Probability for data/dominik/cua-db.db\t=\t780.2738393819244\nLog-Probability for data/sabrina/cua-db.db\t=\t182.19337166401775\nLog-Probability for data/alexander/cua-db.db\t=\t334.70788711871916\nLog-Probability for data/manuela/cua-db.db\t=\t254.84415867771028\nLog-Probability for data/nadine/cua-db.db\t=\t283.2165374933417\nLog-Probability for data/oehlboeck/cua-db.db\t=\t245.320798875786\n----------------------------------\nVerifying data/oehlboeck/cua-db.db\nLog-Probability for data/dominik/cua-db.db\t=\t666.7116190921083\nLog-Probability for data/sabrina/cua-db.db\t=\t116.5121392133144\nLog-Probability for data/alexander/cua-db.db\t=\t284.99601054717624\nLog-Probability for data/manuela/cua-db.db\t=\t197.94209047806152\nLog-Probability for data/nadine/cua-db.db\t=\t264.074543573301\nLog-Probability for data/oehlboeck/cua-db.db\t=\t197.09375788930822\n----------------------------------\n"
    }
   ],
   "source": [
    "datasets = ['data/train/cua-db.db', 'data/test1/cua-db.db', 'data/test2/cua-db.db', 'data/test3/cua-db.db', 'data/test4/cua-db.db', 'data/test5/cua-db.db']\n",
    "\n",
    "models = []\n",
    "max_states = 10\n",
    "\n",
    "print('------------------------------------------------')\n",
    "print(f'Training model with up to {max_states} states!')\n",
    "print('------------------------------------------------')\n",
    "\n",
    "for dataset in datasets:\n",
    "    dataset, model, validate = trainModel(dataset, features =  ['x_position', 'y_position'], max_states = max_states)\n",
    "    models.append([dataset, model, validate])\n",
    "\n",
    "print('------------------------------------------------')\n",
    "print(f'Now verifying model with validation set!')\n",
    "print('------------------------------------------------')\n",
    "\n",
    "for m in models:\n",
    "    dataset = m[0]\n",
    "    model = m[1]\n",
    "    validate = m[2]\n",
    "\n",
    "    print(f'Verifying {dataset}')\n",
    "\n",
    "    for n in models:\n",
    "        dataset_validate = n[0]\n",
    "        validate = n[2]\n",
    "        log_prob = sum(model.log_probability(sequence) / len(sequence) for sequence in validate)\n",
    "        print(f'Log-Probability for {dataset_validate}\\t=\\t{log_prob}')\n",
    "    \n",
    "    print('----------------------------------')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Checking why some log_proability calls return NaN\n",
    "e.g. when using [data/sabrina/cua-db.db]: Trained model with 7 states | Log-probability on test set: nan\n",
    "\n",
    "Solution: This does not depend on the data that is used, because in some instances the training works just fine, whereas in other instances it is possible to fail. This is by random so it might be a bug in the library / or some sort of wrong initializiation, or just an implementation detail, that a solution is not found every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Train len: 77 | Test len: 22 | Validate len: 12\n-117.5650259480935\n-118.66991525099479\nnan\n-118.53140852129953\n-118.16725629380666\n-112.7903271858375\n-118.66991523282542\n-118.66991415338306\n-112.70555901730881\n-112.54481101739562\n"
    }
   ],
   "source": [
    "from pomegranate import *\n",
    "import numpy as np\n",
    "\n",
    "strokes = getCombinedStrokeSequencesFromDataFrames(['data/test5/cua-db.db'], features =  ['x_position', 'y_position'])\n",
    "train, test, validate = generateTrainTestValidateSplit(strokes)\n",
    "states = 7\n",
    "\n",
    "print(f'Train len: {len(train)} | Test len: {len(test)} | Validate len: {len(validate)}')\n",
    "\n",
    "weights = []\n",
    "\n",
    "for seq in train:\n",
    "    weights.append(len(seq))\n",
    "\n",
    "for i in range(10):\n",
    "    model, history = HiddenMarkovModel.from_samples(MultivariateGaussianDistribution,\n",
    "        n_components = states,\n",
    "        X = train,\n",
    "        weights = weights,\n",
    "        algorithm = 'baum-welch',\n",
    "        use_pseudocount = False,\n",
    "        n_init = 10,\n",
    "        return_history = True,\n",
    "        verbose = False)\n",
    "\n",
    "    log_prob = sum(model.log_probability(sequence) / len(sequence) for sequence in validate)\n",
    "    print(log_prob)\n",
    "\n",
    "    #if np.isnan(history.total_improvement[len(history.total_improvement) - 1]):\n",
    "    #    print('Did not work')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}